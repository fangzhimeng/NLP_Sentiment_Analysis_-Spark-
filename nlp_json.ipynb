{"cells":[{"cell_type":"markdown","source":["# Sentiment Analysis of Amazon Customer Reviews"],"metadata":{}},{"cell_type":"markdown","source":["This project uses the customer review data from Amazon.com Kindle store to perform a supervised binary (positive or negative) sentiment classification analysis. We use various data pre-processing techniques and three machine learning models, namely, Naive Bayes classification model, the Logistic regression model, and the linear support vector classification model. The result provides 87% prediction accuracy."],"metadata":{}},{"cell_type":"code","source":["import pyspark\nspark.conf.set('spark.sql.shuffle.partitions', '8')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Load dataset\nThe data comes from the website \"Amazon product data\" (http://jmcauley.ucsd.edu/data/amazon/) managed by Dr. Julian McAuley from UCSD. We choose the smaller subset of the customer review data from the Kindle store of Amazon.com. The data is in the JSON format, which contains 982,619 reviews and metadata spanning May 1996 - July 2014."],"metadata":{}},{"cell_type":"code","source":["# load original .json data\nkindle_json = spark.read.json('/FileStore/tables/Kindle_Store_5.json')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(kindle_json)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Generate Sentiment Label\n\nReviews with overall rating of 1, 2, or 3 are labeled as negative (label=1), and reviews with overall rating of 4 or 5 are labeled as positive (label=0)."],"metadata":{}},{"cell_type":"code","source":["kindle_json.createOrReplaceTempView('kindle_json_view')\n\ndata_json = spark.sql('''\n  SELECT CASE WHEN overall<4 THEN 1\n          ELSE 0\n          END as label,\n        reviewText as text\n  FROM kindle_json_view\n  WHERE length(reviewText)>2''')\n\ndata_json.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Generate the dataset for modeling\nWe only sample a small portion of the data for demonstration and try to balance the two classes."],"metadata":{}},{"cell_type":"code","source":["# Sampling data\npos = data_json.where('label=0').sample(False, 0.05, seed=1220)\nneg = data_json.where('label=1').sample(False, 0.25, seed=1220)\ndata = pos.union(neg)\ndata.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Negative reviews are on average longer than the positive reviews, but not significantly longer\nfrom pyspark.sql.functions import length\ndata.withColumn('review_length', length('text')).groupBy('label').avg('review_length').show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Data Preprocessing\nData preprocessing process uses the following steps:\n\n* Use HTMLParser to un-escape the text\n* Change \"can't\" to \"can not\", and change \"n't\" to \"not\" (This is useful for the negation handling process)\n* Pad punctuations with blanks\n* Lowercase every word\n* Word tokenization\n* Word lemmatization\n* Perform **negation handling**\n    * Use a state variable to store the negation state\n    * Transform a word followed by a \"not\" or \"no\" into “not_” + word\n    * Whenever the negation state variable is set, the words read are treated as “not_” + word\n    * The state variable is reset when a punctuation mark is encountered or when there is double negation\n* Use **bigram** and/or **trigram** models"],"metadata":{}},{"cell_type":"code","source":["# Define preprocessing function\ndef clean(text):\n    import html\n    import string\n    import nltk\n    nltk.download('wordnet')\n    \n    line = html.unescape(text)\n    line = line.replace(\"can't\", 'can not')\n    line = line.replace(\"n't\", \" not\")\n    # Pad punctuations with white spaces\n    pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}) \n    line = line.translate(pad_punct)\n    line = line.lower()\n    line = line.split() \n    lemmatizer = nltk.WordNetLemmatizer()\n    line = [lemmatizer.lemmatize(t) for t in line] \n    \n    # Negation handling\n    # Add \"not_\" prefix to words behind \"not\", or \"no\" until the end of the sentence\n    tokens = []\n    negated = False\n    for t in line:\n        if t in ['not', 'no']:\n            negated = not negated\n        elif t in string.punctuation or not t.isalpha():\n            negated = False\n        else:\n            tokens.append('not_' + t if negated else t)\n    \n    invalidChars = str(string.punctuation.replace(\"_\", \"\"))  \n    bi_tokens = list(nltk.bigrams(line))\n    bi_tokens = list(map('_'.join, bi_tokens))\n    bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n    tri_tokens = list(nltk.trigrams(line))\n    tri_tokens = list(map('_'.join, tri_tokens))\n    tri_tokens = [i for i in tri_tokens if all(j not in invalidChars for j in i)]\n    tokens = tokens + bi_tokens + tri_tokens      \n    \n    return tokens"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# An example: how the function clean() pre-processes the input text\nexample = clean(\"I don't think this book has any decent information!!! It is full of typos and factual errors that I can't ignore.\")\nprint(example)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Perform data preprocessing\nfrom pyspark.sql.functions import udf, col, size\nfrom pyspark.sql.types import ArrayType, StringType\nclean_udf = udf(clean, ArrayType(StringType()))\ndata_tokens = data.withColumn('tokens', clean_udf(col('text')))\ndata_tokens.show(3)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Split dataset to training (70%) and testing (30%) sets"],"metadata":{}},{"cell_type":"code","source":["# Split data to 70% for training and 30% for testing\ntraining, testing = data_tokens.randomSplit([0.7,0.3], seed=1220)\ntraining.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["training.cache()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Naive Bayes Model (with parameter tuning)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer, IDF\nfrom pyspark.ml import Pipeline\n\ncount_vec = CountVectorizer(inputCol='tokens', outputCol='c_vec', minDF=5.0)\nidf = IDF(inputCol=\"c_vec\", outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Naive Bayes model\nfrom pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes()\n\npipeline_nb = Pipeline(stages=[count_vec, idf, nb])\n\nmodel_nb = pipeline_nb.fit(training)\ntest_nb = model_nb.transform(testing)\ntest_nb.show(3)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Naive Bayes model performance (using default parameters)\n* Area under the ROC curve: 0.8551\n* Accuracy: 0.8553"],"metadata":{}},{"cell_type":"code","source":["# Naive Bayes model ROC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nroc_nb_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\nroc_nb = roc_nb_eval.evaluate(test_nb)\nprint(\"ROC of the NB model: {}\".format(roc_nb))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Naive Bayes model accuracy\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nacc_nb_eval = MulticlassClassificationEvaluator(metricName='accuracy')\nacc_nb = acc_nb_eval.evaluate(test_nb)\nprint(\"Accuracy of the NB model: {}\".format(acc_nb))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Naive Bayes model performance after parameter tuning\n* CountVectorizer.minDF = 7.0\n* NaiveBayes.smooting = 1.0\n* Accuracy: 0.8568 (increased from 0.8553)"],"metadata":{}},{"cell_type":"code","source":["# NB parameter tuning and CV\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid_nb = (ParamGridBuilder()\n                .addGrid(count_vec.minDF, [3.0, 5.0, 7.0, 10.0, 15.0])\n                .addGrid(nb.smoothing, [0.1, 0.5, 1.0])\n                .build())\ncv_nb = CrossValidator(estimator=pipeline_nb, estimatorParamMaps=paramGrid_nb, evaluator=acc_nb_eval, numFolds=5)\ncv_model_nb = cv_nb.fit(training) "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["test_cv_nb = cv_model_nb.transform(testing)\nacc_nb_cv = acc_nb_eval.evaluate(test_cv_nb)\nprint(\"Accuracy of the NB CV model: {}\".format(acc_nb_cv))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["cv_model_nb.bestModel.stages[0].extractParamMap()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["cv_model_nb.bestModel.stages[2].extractParamMap()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Logistic Regressions\nModel performance (using default parameters)\n* Area under the ROC curve: 0.8601\n* Accuracy: 0.8610"],"metadata":{}},{"cell_type":"code","source":["# Logistic Regression model\nfrom pyspark.ml.classification import LogisticRegression\nlgr = LogisticRegression(maxIter=5)\npipeline_lgr = Pipeline(stages=[count_vec, idf, lgr])\n\nmodel_lgr = pipeline_lgr.fit(training)\ntest_lgr = model_lgr.transform(testing)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Logistic Regression model ROC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nroc_lgr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\nroc_lgr = roc_lgr_eval.evaluate(test_lgr)\nprint(\"ROC of the model: {}\".format(roc_lgr))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Logistic Regression model accuracy\n#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nacc_lgr_eval = MulticlassClassificationEvaluator(metricName='accuracy')\nacc_lgr = acc_lgr_eval.evaluate(test_lgr)\nprint(\"Accuracy of the model: {}\".format(acc_lgr))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Linear SVC Model\nModel performance (using default parameters)\n* Area under the ROC curve: 0.8649\n* Accuracy: 0.8656"],"metadata":{}},{"cell_type":"code","source":["# Linear SVC model\nfrom pyspark.ml.classification import LinearSVC\nlsvc = LinearSVC(maxIter=5)\npipeline_lsvc = Pipeline(stages=[count_vec, idf, lsvc])\n\nmodel_lsvc = pipeline_lsvc.fit(training)\ntest_lsvc = model_lsvc.transform(testing)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Linear SVC model ROC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nroc_lsvc_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\nroc_lsvc = roc_lsvc_eval.evaluate(test_lsvc)\nprint(\"ROC of the model: {}\".format(roc_lsvc))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Linear SVC model accuracy\n#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nacc_lsvc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\nacc_lsvc = acc_lsvc_eval.evaluate(test_lsvc)\nprint(\"Accuracy of the model: {}\".format(acc_lsvc))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Predict on new reviews:\nTo demonstrate the model prediction on new review texts, I randomly choose five reviews from the Kindle book *The Brave Ones: A Memoir of Hope, Pride and Military Service, by Michael J. MacLeod*. \n\nThe suffixes \"_1\", \"_2\", ..., \"_5\" indicate the real overall review stars 1, 2, ..., 5.\n\nThe model currectly predicts the first three reviews as \"negative\" (label=1), and the last two as \"positive\" (label=0)."],"metadata":{}},{"cell_type":"code","source":["review_1 = [\"WOW!!! No words describe how bland this book is. It took me a lot to even pick up to read. I would definitely not recommend this book.\"]"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["review_2 = [\"A first person account of the war in Afghanistan. It skipps around a lot and is like a never-ending news article. On the positive side, you do get a feel for what desert fighting is like from a soldiers point of view.\"]"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["review_3 = [\"I liked the premise and most of the book. At the end parts I lost a little interest because I lost the thread of who was who. War is hell. MacLeod did his service unlike most of us.\"]"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["review_4 = [\"Very informative first person account of the the daily life of a US Paratrooper. From training to deployment in combat situations in Afghanistan. Well worth the read and makes you really understand and appreciate their sacrifices\"]"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["review_5 = [\"This is perhaps the best wrote book I have ever read. Articulate and thought provoking. Not just a riveting account of actual combat, but Michael was able to do what few before him have...captured the essence of what one feels as the battle unfolds. Perhaps most of all, I am grateful to call this author 'Fellow Warrior' Airborne all the way!!!\"]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([StructField(\"text\", StringType(), True)])\n\ntext = [review_1, review_2, review_3, review_4, review_5]\nreview_new = spark.createDataFrame(text, schema=schema)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Data preprocessing\nreview_new_tokens = review_new.withColumn('tokens', clean_udf(col('text')))\nreview_new_tokens.show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Prediction using tuned Naive Bayes model\nresult = cv_model_nb.transform(review_new_tokens)\nresult.select('text', 'prediction').show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47}],"metadata":{"name":"nlp_json","notebookId":190787089418947},"nbformat":4,"nbformat_minor":0}
