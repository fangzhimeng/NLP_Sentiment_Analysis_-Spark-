{"cells":[{"cell_type":"code","source":["import pyspark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark.conf.set('spark.sql.shuffle.partitions', '4')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# load original .json data\ndata_json = spark.sql('''\n  SELECT CASE WHEN overall<4 THEN 1\n          ELSE 0\n          END as class,\n        reviewText as text\n  FROM kindle_store_5_json\n  WHERE length(reviewText)>2''')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#pos = data_json.where('class=0').sample(False, 0.2, seed=1220)\n#neg = data_json.where('class=1')\n#data = pos.union(neg)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Sampling data\npos = data_json.where('class=0').sample(False, 0.05, seed=1220)\nneg = data_json.where('class=1').sample(False, 0.25, seed=1220)\ndata = pos.union(neg)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["data.groupBy('class').count().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Define preprocessing function\nimport html\nimport string\ndef clean(row):\n    line = html.unescape(row.text)\n    line = line.replace(\"can't\", 'can not')\n    line = line.replace(\"n't\", \" not\")\n    # Pad punctuations with white spaces\n    pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}) \n    line = line.translate(pad_punct)\n    line = line.lower()\n    line = line.split()\n    \n    # Negation handling\n    # Add \"not_\" prefix to words behind \"not\", or \"no\" until the end of the sentence\n    tokens = []\n    negated = False\n    for t in line:\n        if t in ['not', 'no']:\n            negated = not negated\n        elif t in string.punctuation or not t.isalpha():\n            negated = False\n        else:\n            tokens.append('not_' + t if negated else t)\n    \n    return tokens"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Preprocessing data\nfrom pyspark.sql.functions import col\ndata_text = data.select('text')\ndata_class = data.select('class')\ndata_text_rdd = data_text.rdd.map(clean)\ndata_text_df = data_text_rdd.zipWithUniqueId().toDF().withColumnRenamed('_1', 'text').withColumnRenamed('_2', 'idx')\ndata_class_df = data_class.rdd.zipWithUniqueId().toDF().withColumnRenamed('_1', 'class').withColumnRenamed('_2', 'idx')\ndata_df = data_class_df.join(data_text_df, on='idx')\ndata_df = data_df.select(col('class').getField('class'), 'text').withColumnRenamed('class.class', 'label')\ndata_df.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Generate bigrams and trigrams\nfrom pyspark.ml.feature import NGram\nngram2 = NGram(n=2, inputCol='text', outputCol='bigram')\nngram3 = NGram(n=3, inputCol='text', outputCol='trigram')\ndata_df = ngram2.transform(data_df)\ndata_df = ngram3.transform(data_df)\ndata_df.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Combine unigram, bigram, and trigram to one column\nfrom itertools import chain\nfrom pyspark.sql.functions import col, udf, size\nfrom pyspark.sql.types import *\n\ndef concat(type):\n    def concat_(*args):\n        return list(chain(*args))\n    return udf(concat_, ArrayType(type))\n\nconcat_string_arrays = concat(StringType())\n\ndata_tokens = data_df.select('label', concat_string_arrays(col(\"text\"), col(\"bigram\"), col(\"trigram\"))).\\\n  withColumnRenamed('concat_(text, bigram, trigram)', 'tokens').\\\n  withColumn('length', size(col('tokens')))\ndata_tokens.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Split data to 70% for training and 30% for testing\ntraining, testing = data_tokens.randomSplit([0.7,0.3])\ntraining.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\nfrom pyspark.ml import Pipeline\n\ncount_vec = CountVectorizer(inputCol='tokens', outputCol='c_vec', minDF=5)\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\nclean_up = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')\ndata_prep_pipe = Pipeline(stages=[count_vec, idf, clean_up])\n\n# Define dictionary using training data\ncleaner = data_prep_pipe.fit(training)\n\n# Feature Extraction for training data\nclean_train = cleaner.transform(training)\nclean_train = clean_train.select(['label','features'])\nclean_train.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Feature Extraction for testing data\nclean_test = cleaner.transform(testing)\nclean_test = clean_test.select(['label','features'])\nclean_test.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Naive Bayes model\nfrom pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes()\nmodel_nb = nb.fit(clean_train)\ntest_nb = model_nb.transform(clean_test)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Naive Bayes model accuracy\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nacc_eval = MulticlassClassificationEvaluator()\nacc_nb = acc_eval.evaluate(test_nb)\nprint(\"Accuracy of the model: {}\".format(acc_nb))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Logistic Regression model\nfrom pyspark.ml.classification import LogisticRegression\nlgr = LogisticRegression(maxIter=5)\nmodel_lgr = lgr.fit(clean_train)\ntest_lgr = model_lgr.transform(clean_test)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Logistic Regression model accuracy\n#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#acc_eval = MulticlassClassificationEvaluator()\nacc_lgr = acc_eval.evaluate(test_lgr)\nprint(\"Accuracy of the model: {}\".format(acc_lgr))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Linear SVC model\nfrom pyspark.ml.classification import LinearSVC\nlsvc = LinearSVC(maxIter=5)\nmodel_lsvc = lsvc.fit(clean_train)\ntest_lsvc = model_lsvc.transform(clean_test)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Linear SVC model accuracy\n#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#acc_eval = MulticlassClassificationEvaluator()\nacc_lsvc = acc_eval.evaluate(test_lsvc)\nprint(\"Accuracy of the model: {}\".format(acc_lsvc))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"nlp_json","notebookId":190787089418947},"nbformat":4,"nbformat_minor":0}
